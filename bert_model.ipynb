{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"bert_model.ipynb","provenance":[],"authorship_tag":"ABX9TyNFx2WH9AG818bK8Bs6Oyj7"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"fkZ0OvDVKikc","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1599912422354,"user_tz":-330,"elapsed":21864,"user":{"displayName":"Bikram Dutta","photoUrl":"","userId":"16881037768699999343"}},"outputId":"c1cdfccb-bc64-4775-8afa-13f6fdee3365"},"source":["# Mount Google Drive\n","from google.colab import drive # import drive from google colab\n","\n","ROOT = \"/content/drive\"     # default location for the drive\n","print(ROOT)                 # print content of ROOT (Optional)\n","\n","drive.mount(ROOT, force_remount=True)           # we mount the google drive at /content/drive"],"execution_count":2,"outputs":[{"output_type":"stream","text":["/content/drive\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"QRNgNaELKPOg","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":391},"executionInfo":{"status":"ok","timestamp":1599912437074,"user_tz":-330,"elapsed":6555,"user":{"displayName":"Bikram Dutta","photoUrl":"","userId":"16881037768699999343"}},"outputId":"c3478ac4-8718-4e7a-eece-5992fbcd1e2b"},"source":["!pip install bert-for-tf2"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Collecting bert-for-tf2\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/af/c1/015648a2186b25c6de79d15bec40d3d946fcf1dd5067d1c1b28009506486/bert-for-tf2-0.14.6.tar.gz (40kB)\n","\r\u001b[K     |████████                        | 10kB 25.3MB/s eta 0:00:01\r\u001b[K     |████████████████                | 20kB 2.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 30kB 3.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 40kB 2.7MB/s \n","\u001b[?25hCollecting py-params>=0.9.6\n","  Downloading https://files.pythonhosted.org/packages/a4/bf/c1c70d5315a8677310ea10a41cfc41c5970d9b37c31f9c90d4ab98021fd1/py-params-0.9.7.tar.gz\n","Collecting params-flow>=0.8.0\n","  Downloading https://files.pythonhosted.org/packages/a9/95/ff49f5ebd501f142a6f0aaf42bcfd1c192dc54909d1d9eb84ab031d46056/params-flow-0.8.2.tar.gz\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (1.18.5)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (4.41.1)\n","Building wheels for collected packages: bert-for-tf2, py-params, params-flow\n","  Building wheel for bert-for-tf2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for bert-for-tf2: filename=bert_for_tf2-0.14.6-cp36-none-any.whl size=30318 sha256=6af0f286184f7676e531f57a1e3e58a37f62cd325f3fae9eacef3741cc883630\n","  Stored in directory: /root/.cache/pip/wheels/07/a0/b4/75b0601ebaa41e517a797fe9cea119c789664c8408f8a74ae9\n","  Building wheel for py-params (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for py-params: filename=py_params-0.9.7-cp36-none-any.whl size=7304 sha256=9ac2f6ac8dd9e8e32297e10963c98c2a1824547dec2fc2eb075cd45cd7cfdad4\n","  Stored in directory: /root/.cache/pip/wheels/67/f5/19/b461849a50aefdf4bab47c4756596e82ee2118b8278e5a1980\n","  Building wheel for params-flow (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for params-flow: filename=params_flow-0.8.2-cp36-none-any.whl size=19475 sha256=9a824839911b9b5866c5dbac218a2701c198a389c19b13fb45ebc863c9fd429d\n","  Stored in directory: /root/.cache/pip/wheels/08/c8/7f/81c86b9ff2b86e2c477e3914175be03e679e596067dc630c06\n","Successfully built bert-for-tf2 py-params params-flow\n","Installing collected packages: py-params, params-flow, bert-for-tf2\n","Successfully installed bert-for-tf2-0.14.6 params-flow-0.8.2 py-params-0.9.7\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"IKbM8VBmKc1V","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1599912437076,"user_tz":-330,"elapsed":701,"user":{"displayName":"Bikram Dutta","photoUrl":"","userId":"16881037768699999343"}},"outputId":"09f95176-1f09-4d80-c3f4-1e6709bfc381"},"source":["%pwd\n","%cd 'drive/My Drive/Workspaces/twitter_sentiment_analysis'"],"execution_count":4,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/Workspaces/twitter_sentiment_analysis\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"z6qWs-ozKIiI","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":136},"executionInfo":{"status":"ok","timestamp":1599912443354,"user_tz":-330,"elapsed":3673,"user":{"displayName":"Bikram Dutta","photoUrl":"","userId":"16881037768699999343"}},"outputId":"6818ccf9-f125-4ea2-d91c-aed198028e3f"},"source":["import nltk\n","import re\n","\n","from nltk.corpus import stopwords\n","from nltk.stem.porter import PorterStemmer\n","from nltk.stem import WordNetLemmatizer \n","\n","import gc\n","import re\n","import string\n","import operator\n","\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# import tokenization\n","from wordcloud import STOPWORDS\n","\n","from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\n","from sklearn.metrics import precision_score, recall_score, f1_score\n","\n","import tensorflow as tf\n","import tensorflow_hub as hub\n","from tensorflow import keras\n","from tensorflow.keras.optimizers import SGD, Adam\n","from tensorflow.keras.layers import Dense, Input, Dropout, GlobalAveragePooling1D\n","from tensorflow.keras.models import Model, Sequential\n","from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, Callback\n","import bert\n","\n","nltk.download('wordnet')\n","nltk.download('stopwords')"],"execution_count":5,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n","  import pandas.util.testing as tm\n"],"name":"stderr"},{"output_type":"stream","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"h8ntyuuqKroc","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1599912446388,"user_tz":-330,"elapsed":1974,"user":{"displayName":"Bikram Dutta","photoUrl":"","userId":"16881037768699999343"}}},"source":["import pandas as pd\n","import numpy as np\n","train_df = pd.read_csv('data/preprocessed_train.csv')\n","test_df = pd.read_csv('data/preprocessed_test.csv')"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"tFlsJ8PPKCyD","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":119},"executionInfo":{"status":"ok","timestamp":1599912777098,"user_tz":-330,"elapsed":783,"user":{"displayName":"Bikram Dutta","photoUrl":"","userId":"16881037768699999343"}},"outputId":"f6b35904-d5e8-4a8e-861e-3bdeb4ca7870"},"source":["HateSpeechTweets = train_df['label'] == 1\n","\n","SEED = 1337\n","K = 2\n","skf = StratifiedKFold(n_splits=K, random_state=SEED, shuffle=True)\n","\n","print('Whole Training Set Shape = {}'.format(train_df.shape))\n","print('Whole Training Set Target Rate (Hate Speech) {}/{} (Not Hate Speech)'.format(train_df[HateSpeechTweets]['label'].count(), train_df[~HateSpeechTweets]['label'].count()))\n","\n","for fold, (trn_idx, val_idx) in enumerate(skf.split(train_df['cleaned_text'], train_df['label']), 1):\n","    print('\\nFold {} Training Set Shape = {} - Validation Set Shape = {}'.format(fold, train_df.loc[trn_idx, 'cleaned_text'].shape, train_df.loc[val_idx, 'cleaned_text'].shape))"],"execution_count":16,"outputs":[{"output_type":"stream","text":["Whole Training Set Shape = (31962, 14)\n","Whole Training Set Target Rate (Hate Speech) 2242/29720 (Not Hate Speech)\n","\n","Fold 1 Training Set Shape = (15981,) - Validation Set Shape = (15981,)\n","\n","Fold 2 Training Set Shape = (15981,) - Validation Set Shape = (15981,)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"vhRYLhkzLQNg","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1599912451662,"user_tz":-330,"elapsed":776,"user":{"displayName":"Bikram Dutta","photoUrl":"","userId":"16881037768699999343"}}},"source":["class ClassificationReport(Callback):\n","  def __init__(self, train_data=(), validation_data=()):\n","    super(Callback, self).__init__()\n","\n","    self.X_train, self.y_train = train_data\n","    self.train_precision_scores = []\n","    self.train_recall_scores = []\n","    self.train_f1_scores = []\n","\n","    self.X_val, self.y_val = validation_data\n","    self.val_precision_scores = []\n","    self.val_recall_scores = []\n","    self.val_f1_scores = [] \n","               \n","  def on_epoch_end(self, epoch, logs={}):    \n","    train_predictions = np.round(self.model.predict(self.X_train, verbose=0))        \n","    train_precision = precision_score(self.y_train, train_predictions, average='macro')\n","    train_recall = recall_score(self.y_train, train_predictions, average='macro')\n","    train_f1 = f1_score(self.y_train, train_predictions, average='macro')\n","    self.train_precision_scores.append(train_precision)        \n","    self.train_recall_scores.append(train_recall)\n","    self.train_f1_scores.append(train_f1)\n","\n","    val_predictions = np.round(self.model.predict(self.X_val, verbose=0))\n","    val_precision = precision_score(self.y_val, val_predictions, average='macro')\n","    val_recall = recall_score(self.y_val, val_predictions, average='macro')\n","    val_f1 = f1_score(self.y_val, val_predictions, average='macro')\n","    self.val_precision_scores.append(val_precision)        \n","    self.val_recall_scores.append(val_recall)        \n","    self.val_f1_scores.append(val_f1)\n","\n","    print('\\nEpoch: {} - Training Precision: {:.6} - Training Recall: {:.6} - Training F1: {:.6}'.format(epoch + 1, train_precision, train_recall, train_f1))\n","    print('Epoch: {} - Validation Precision: {:.6} - Validation Recall: {:.6} - Validation F1: {:.6}'.format(epoch + 1, val_precision, val_recall, val_f1))  "],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"u4aTqWoJLrAq","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1599912471674,"user_tz":-330,"elapsed":17595,"user":{"displayName":"Bikram Dutta","photoUrl":"","userId":"16881037768699999343"}},"outputId":"eecdf023-d916-4281-e2e9-efd5d1d485be"},"source":["%%time\n","bert_layer = hub.KerasLayer('https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1', trainable=True)"],"execution_count":9,"outputs":[{"output_type":"stream","text":["CPU times: user 9.73 s, sys: 1.7 s, total: 11.4 s\n","Wall time: 16.5 s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6dLY6KwMUgTW","colab_type":"code","colab":{}},"source":["train_df['cleaned_text'] = train_df.cleaned_text.apply(lambda text : text.strip())\n","test_df['cleaned_text'] = test_df.cleaned_text.apply(lambda text : text.strip())\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sBdVL4CwLuxJ","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1599914570674,"user_tz":-330,"elapsed":1724,"user":{"displayName":"Bikram Dutta","photoUrl":"","userId":"16881037768699999343"}}},"source":["class DisasterDetector:\n","  def __init__(self, bert_layer, max_seq_length=128, lr=0.0001, epochs=15, batch_size=32):        \n","    # BERT and Tokenization params\n","    self.bert_layer = bert_layer\n","\n","    self.max_seq_length = max_seq_length        \n","    vocab_file = self.bert_layer.resolved_object.vocab_file.asset_path.numpy()\n","    do_lower_case = self.bert_layer.resolved_object.do_lower_case.numpy()\n","    self.tokenizer = bert.bert_tokenization.FullTokenizer(vocab_file, do_lower_case)\n","\n","    # Learning control params\n","    self.lr = lr\n","    self.epochs = epochs\n","    self.batch_size = batch_size\n","\n","    self.models = []\n","    self.scores = {}\n","\n","  def encode(self, texts):\n","\n","    all_tokens = []\n","    all_masks = []\n","    all_segments = []\n","\n","    for text in texts:\n","      print(text)\n","      text = self.tokenizer.tokenize(text)\n","      text = text[:self.max_seq_length - 2]\n","      input_sequence = ['[CLS]'] + text + ['[SEP]']\n","      pad_len = self.max_seq_length - len(input_sequence)\n","\n","      tokens = self.tokenizer.convert_tokens_to_ids(input_sequence)\n","      tokens += [0] * pad_len\n","      pad_masks = [1] * len(input_sequence) + [0] * pad_len\n","      segment_ids = [0] * self.max_seq_length\n","\n","      all_tokens.append(tokens)\n","      all_masks.append(pad_masks)\n","      all_segments.append(segment_ids)\n","\n","    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)\n","\n","\n","  def build_model(self):\n","    input_word_ids = Input(shape=(self.max_seq_length,), dtype=tf.int32, name='input_word_ids')\n","    input_mask = Input(shape=(self.max_seq_length,), dtype=tf.int32, name='input_mask')\n","    segment_ids = Input(shape=(self.max_seq_length,), dtype=tf.int32, name='segment_ids')    \n","\n","    pooled_output, sequence_output = self.bert_layer([input_word_ids, input_mask, segment_ids])   \n","    clf_output = sequence_output[:, 0, :]\n","    out = Dense(1, activation='sigmoid')(clf_output)\n","\n","    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n","    optimizer = SGD(learning_rate=self.lr, momentum=0.8)\n","    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n","\n","    return model\n","\n","\n","  def train(self, X):\n","\n","    for fold, (trn_idx, val_idx) in enumerate(skf.split(X['cleaned_text'], X['hashTags'])):\n","      print('\\nFold {}\\n'.format(fold))\n","      print(trn_idx)\n","      X_trn_encoded = self.encode(X.loc[trn_idx, 'cleaned_text'].str.lower())\n","      y_trn = X.loc[trn_idx, 'label']\n","      X_val_encoded = self.encode(X.loc[val_idx, 'cleaned_text'].str.lower())\n","      y_val = X.loc[val_idx, 'label']\n","\n","      # Callbacks\n","      metrics = ClassificationReport(train_data=(X_trn_encoded, y_trn), validation_data=(X_val_encoded, y_val))\n","\n","      # Model\n","      model = self.build_model()        \n","      model.fit(X_trn_encoded, y_trn, validation_data=(X_val_encoded, y_val), callbacks=[metrics], epochs=self.epochs, batch_size=self.batch_size)\n","\n","      self.models.append(model)\n","      self.scores[fold] = {\n","          'train': {\n","              'precision': metrics.train_precision_scores,\n","              'recall': metrics.train_recall_scores,\n","              'f1': metrics.train_f1_scores                    \n","          },\n","          'validation': {\n","              'precision': metrics.val_precision_scores,\n","              'recall': metrics.val_recall_scores,\n","              'f1': metrics.val_f1_scores                    \n","          }\n","      }\n","\n","\n","  def plot_learning_curve(self):\n","    fig, axes = plt.subplots(nrows=K, ncols=2, figsize=(20, K * 6), dpi=100)\n","\n","    for i in range(K):\n","      # Classification Report curve\n","      sns.lineplot(x=np.arange(1, self.epochs + 1), y=clf.models[i].history.history['val_accuracy'], ax=axes[i][0], label='val_accuracy')\n","      sns.lineplot(x=np.arange(1, self.epochs + 1), y=clf.scores[i]['validation']['precision'], ax=axes[i][0], label='val_precision')\n","      sns.lineplot(x=np.arange(1, self.epochs + 1), y=clf.scores[i]['validation']['recall'], ax=axes[i][0], label='val_recall')\n","      sns.lineplot(x=np.arange(1, self.epochs + 1), y=clf.scores[i]['validation']['f1'], ax=axes[i][0], label='val_f1')        \n","\n","      axes[i][0].legend() \n","      axes[i][0].set_title('Fold {} Validation Classification Report'.format(i), fontsize=14)\n","\n","      # Loss curve\n","      sns.lineplot(x=np.arange(1, self.epochs + 1), y=clf.models[0].history.history['loss'], ax=axes[i][1], label='train_loss')\n","      sns.lineplot(x=np.arange(1, self.epochs + 1), y=clf.models[0].history.history['val_loss'], ax=axes[i][1], label='val_loss')\n","\n","      axes[i][1].legend() \n","      axes[i][1].set_title('Fold {} Train / Validation Loss'.format(i), fontsize=14)\n","\n","      for j in range(2):\n","        axes[i][j].set_xlabel('Epoch', size=12)\n","        axes[i][j].tick_params(axis='x', labelsize=12)\n","        axes[i][j].tick_params(axis='y', labelsize=12)\n","\n","        plt.show()\n","\n","\n","  def predict(self, X):\n","    X_test_encoded = self.encode(X['cleaned_text'].str.lower())\n","    y_pred = np.zeros((X_test_encoded[0].shape[0], 1))\n","\n","    for model in self.models:\n","      y_pred += model.predict(X_test_encoded) / len(self.models)\n","\n","    return y_pred"],"execution_count":38,"outputs":[]},{"cell_type":"code","metadata":{"id":"SddH_mVuNMkY","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":323},"executionInfo":{"status":"error","timestamp":1599914575884,"user_tz":-330,"elapsed":1669,"user":{"displayName":"Bikram Dutta","photoUrl":"","userId":"16881037768699999343"}},"outputId":"18db1358-06b0-479e-fd9d-75ad0266587a"},"source":["clf = DisasterDetector(bert_layer, max_seq_length=128, lr=0.0001, epochs=10, batch_size=32)\n","clf.train(train_df)"],"execution_count":39,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-39-3aa140be90b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDisasterDetector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_seq_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-38-2d3cff3268a5>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     60\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mfold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrn_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_idx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mskf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cleaned_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'hashTags'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nFold {}\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrn_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36msplit\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m    728\u001b[0m         \u001b[0mto\u001b[0m \u001b[0man\u001b[0m \u001b[0minteger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m         \"\"\"\n\u001b[0;32m--> 730\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    576\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m             _assert_all_finite(array,\n\u001b[0;32m--> 578\u001b[0;31m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[0m\u001b[1;32m    579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mensure_min_samples\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'object'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mallow_nan\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_object_dtype_isnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Input contains NaN\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Input contains NaN"]}]}]}